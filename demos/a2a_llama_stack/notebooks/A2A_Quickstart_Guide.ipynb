{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fef1ba28",
   "metadata": {},
   "source": [
    "# A simple agent A2A Application with Custom Tools\n",
    "\n",
    "This notebook presents a simple scenario where an agent uses the A2A protocol to query another agent for information using custom tools. We show how to initialize an agent in Llama Stack and grant it access to communicating with another, external agent.\n",
    "\n",
    "This demo demonstrates core A2A communication principles.\n",
    "\n",
    "## Overview\n",
    "\n",
    "This notebook covers the following steps:\n",
    "\n",
    "1. Setting up a Llama Stack agent with custom tool capabilities (e.g., random number generation, date retrieval).\n",
    "2. Serving this agent over an A2A server.\n",
    "3. Initializing another Llama Stack agent capable of communicating with the custom tool agent.\n",
    "4. Launching the second agent and using it to answer user queries by leveraging the first agent's tools.\n",
    "\n",
    "## Prerequisites\n",
    "\n",
    "Before starting, ensure you have the following:\n",
    "- `python_requires >= 3.11`\n",
    "\n",
    "- Followed the instructions in the [Setup Guide](../../rag_agentic/notebooks/Level0_getting_started_with_Llama_Stack.ipynb) notebook.\n",
    "\n",
    "## Additional environment variables\n",
    "This demo requires the following environment variables in addition to those defined in the [Setup Guide](../../rag_agentic/notebooks//Level0_getting_started_with_Llama_Stack.ipynb):\n",
    "- `CUSTOM_TOOL_AGENT_LOCAL_PORT`: the port over which we will serve the exported A2A agent with custom tool capabilities."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4c88e09",
   "metadata": {},
   "source": [
    "## 1. Setting Up this Notebook\n",
    "To provide A2A communication capabilities, we will use the [sample implementation by Google](https://github.com/google/A2A/tree/main/samples/python). Please make sure that the content of the referenced directory is available on your Python path. This can be done, for example, by running the following command:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "01c195db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fatal: destination path 'a2a-samples' already exists and is not an empty directory.\n",
      "Collecting annotated-types==0.7.0 (from -r ../requirements.txt (line 1))\n",
      "  Using cached annotated_types-0.7.0-py3-none-any.whl.metadata (15 kB)\n",
      "Collecting anyio==4.9.0 (from -r ../requirements.txt (line 2))\n",
      "  Using cached anyio-4.9.0-py3-none-any.whl.metadata (4.7 kB)\n",
      "Requirement already satisfied: appnope==0.1.4 in /Users/kcogan/Documents/llama-stack-on-ocp/venv/lib/python3.11/site-packages (from -r ../requirements.txt (line 3)) (0.1.4)\n",
      "Requirement already satisfied: asttokens==3.0.0 in /Users/kcogan/Documents/llama-stack-on-ocp/venv/lib/python3.11/site-packages (from -r ../requirements.txt (line 4)) (3.0.0)\n",
      "Collecting asyncclick==8.1.8 (from -r ../requirements.txt (line 5))\n",
      "  Using cached asyncclick-8.1.8.0-py3-none-any.whl.metadata (2.6 kB)\n",
      "Collecting certifi==2025.1.31 (from -r ../requirements.txt (line 6))\n",
      "  Using cached certifi-2025.1.31-py3-none-any.whl.metadata (2.5 kB)\n",
      "Collecting cffi==1.17.1 (from -r ../requirements.txt (line 7))\n",
      "  Using cached cffi-1.17.1-cp311-cp311-macosx_11_0_arm64.whl.metadata (1.5 kB)\n",
      "Collecting charset-normalizer==3.4.2 (from -r ../requirements.txt (line 8))\n",
      "  Using cached charset_normalizer-3.4.2-cp311-cp311-macosx_10_9_universal2.whl.metadata (35 kB)\n",
      "Collecting click==8.1.8 (from -r ../requirements.txt (line 9))\n",
      "  Using cached click-8.1.8-py3-none-any.whl.metadata (2.3 kB)\n",
      "Requirement already satisfied: comm==0.2.2 in /Users/kcogan/Documents/llama-stack-on-ocp/venv/lib/python3.11/site-packages (from -r ../requirements.txt (line 10)) (0.2.2)\n",
      "Collecting cryptography==45.0.3 (from -r ../requirements.txt (line 11))\n",
      "  Using cached cryptography-45.0.3-cp311-abi3-macosx_10_9_universal2.whl.metadata (5.7 kB)\n",
      "Requirement already satisfied: debugpy==1.8.14 in /Users/kcogan/Documents/llama-stack-on-ocp/venv/lib/python3.11/site-packages (from -r ../requirements.txt (line 12)) (1.8.14)\n",
      "Requirement already satisfied: decorator==5.2.1 in /Users/kcogan/Documents/llama-stack-on-ocp/venv/lib/python3.11/site-packages (from -r ../requirements.txt (line 13)) (5.2.1)\n",
      "Collecting distro==1.9.0 (from -r ../requirements.txt (line 14))\n",
      "  Using cached distro-1.9.0-py3-none-any.whl.metadata (6.8 kB)\n",
      "Collecting dotenv==0.9.9 (from -r ../requirements.txt (line 15))\n",
      "  Using cached dotenv-0.9.9-py2.py3-none-any.whl.metadata (279 bytes)\n",
      "Requirement already satisfied: executing==2.2.0 in /Users/kcogan/Documents/llama-stack-on-ocp/venv/lib/python3.11/site-packages (from -r ../requirements.txt (line 16)) (2.2.0)\n",
      "Collecting fire==0.7.0 (from -r ../requirements.txt (line 17))\n",
      "  Using cached fire-0.7.0-py3-none-any.whl\n",
      "Collecting h11==0.16.0 (from -r ../requirements.txt (line 18))\n",
      "  Using cached h11-0.16.0-py3-none-any.whl.metadata (8.3 kB)\n",
      "Collecting httpcore==1.0.9 (from -r ../requirements.txt (line 19))\n",
      "  Using cached httpcore-1.0.9-py3-none-any.whl.metadata (21 kB)\n",
      "Collecting httpx==0.28.1 (from -r ../requirements.txt (line 20))\n",
      "  Using cached httpx-0.28.1-py3-none-any.whl.metadata (7.1 kB)\n",
      "Collecting httpx-sse==0.4.0 (from -r ../requirements.txt (line 21))\n",
      "  Using cached httpx_sse-0.4.0-py3-none-any.whl.metadata (9.0 kB)\n",
      "Collecting idna==3.10 (from -r ../requirements.txt (line 22))\n",
      "  Using cached idna-3.10-py3-none-any.whl.metadata (10 kB)\n",
      "Requirement already satisfied: ipykernel==6.29.5 in /Users/kcogan/Documents/llama-stack-on-ocp/venv/lib/python3.11/site-packages (from -r ../requirements.txt (line 23)) (6.29.5)\n",
      "Requirement already satisfied: ipython==9.3.0 in /Users/kcogan/Documents/llama-stack-on-ocp/venv/lib/python3.11/site-packages (from -r ../requirements.txt (line 24)) (9.3.0)\n",
      "Requirement already satisfied: ipython_pygments_lexers==1.1.1 in /Users/kcogan/Documents/llama-stack-on-ocp/venv/lib/python3.11/site-packages (from -r ../requirements.txt (line 25)) (1.1.1)\n",
      "Requirement already satisfied: jedi==0.19.2 in /Users/kcogan/Documents/llama-stack-on-ocp/venv/lib/python3.11/site-packages (from -r ../requirements.txt (line 26)) (0.19.2)\n",
      "Requirement already satisfied: jupyter_client==8.6.3 in /Users/kcogan/Documents/llama-stack-on-ocp/venv/lib/python3.11/site-packages (from -r ../requirements.txt (line 27)) (8.6.3)\n",
      "Requirement already satisfied: jupyter_core==5.8.1 in /Users/kcogan/Documents/llama-stack-on-ocp/venv/lib/python3.11/site-packages (from -r ../requirements.txt (line 28)) (5.8.1)\n",
      "Collecting jwcrypto==1.5.6 (from -r ../requirements.txt (line 29))\n",
      "  Using cached jwcrypto-1.5.6-py3-none-any.whl.metadata (3.1 kB)\n",
      "Collecting llama_stack_client==0.2.2 (from -r ../requirements.txt (line 30))\n",
      "  Using cached llama_stack_client-0.2.2-py3-none-any.whl.metadata (15 kB)\n",
      "Collecting markdown-it-py==3.0.0 (from -r ../requirements.txt (line 31))\n",
      "  Using cached markdown_it_py-3.0.0-py3-none-any.whl.metadata (6.9 kB)\n",
      "Requirement already satisfied: matplotlib-inline==0.1.7 in /Users/kcogan/Documents/llama-stack-on-ocp/venv/lib/python3.11/site-packages (from -r ../requirements.txt (line 32)) (0.1.7)\n",
      "Collecting mdurl==0.1.2 (from -r ../requirements.txt (line 33))\n",
      "  Using cached mdurl-0.1.2-py3-none-any.whl.metadata (1.6 kB)\n",
      "Requirement already satisfied: nest-asyncio==1.6.0 in /Users/kcogan/Documents/llama-stack-on-ocp/venv/lib/python3.11/site-packages (from -r ../requirements.txt (line 34)) (1.6.0)\n",
      "Collecting numpy==2.2.5 (from -r ../requirements.txt (line 35))\n",
      "  Using cached numpy-2.2.5-cp311-cp311-macosx_14_0_arm64.whl.metadata (62 kB)\n",
      "Requirement already satisfied: packaging==25.0 in /Users/kcogan/Documents/llama-stack-on-ocp/venv/lib/python3.11/site-packages (from -r ../requirements.txt (line 36)) (25.0)\n",
      "Collecting pandas==2.2.3 (from -r ../requirements.txt (line 37))\n",
      "  Using cached pandas-2.2.3-cp311-cp311-macosx_11_0_arm64.whl.metadata (89 kB)\n",
      "Requirement already satisfied: parso==0.8.4 in /Users/kcogan/Documents/llama-stack-on-ocp/venv/lib/python3.11/site-packages (from -r ../requirements.txt (line 38)) (0.8.4)\n",
      "Requirement already satisfied: pexpect==4.9.0 in /Users/kcogan/Documents/llama-stack-on-ocp/venv/lib/python3.11/site-packages (from -r ../requirements.txt (line 39)) (4.9.0)\n",
      "Requirement already satisfied: platformdirs==4.3.8 in /Users/kcogan/Documents/llama-stack-on-ocp/venv/lib/python3.11/site-packages (from -r ../requirements.txt (line 40)) (4.3.8)\n",
      "Requirement already satisfied: prompt_toolkit==3.0.51 in /Users/kcogan/Documents/llama-stack-on-ocp/venv/lib/python3.11/site-packages (from -r ../requirements.txt (line 41)) (3.0.51)\n",
      "Requirement already satisfied: psutil==7.0.0 in /Users/kcogan/Documents/llama-stack-on-ocp/venv/lib/python3.11/site-packages (from -r ../requirements.txt (line 42)) (7.0.0)\n",
      "Requirement already satisfied: ptyprocess==0.7.0 in /Users/kcogan/Documents/llama-stack-on-ocp/venv/lib/python3.11/site-packages (from -r ../requirements.txt (line 43)) (0.7.0)\n",
      "Requirement already satisfied: pure_eval==0.2.3 in /Users/kcogan/Documents/llama-stack-on-ocp/venv/lib/python3.11/site-packages (from -r ../requirements.txt (line 44)) (0.2.3)\n",
      "Collecting pyaml==25.1.0 (from -r ../requirements.txt (line 45))\n",
      "  Using cached pyaml-25.1.0-py3-none-any.whl.metadata (12 kB)\n",
      "Collecting pycparser==2.22 (from -r ../requirements.txt (line 46))\n",
      "  Using cached pycparser-2.22-py3-none-any.whl.metadata (943 bytes)\n",
      "Collecting pydantic==2.11.3 (from -r ../requirements.txt (line 47))\n",
      "  Using cached pydantic-2.11.3-py3-none-any.whl.metadata (65 kB)\n",
      "Collecting pydantic_core==2.33.1 (from -r ../requirements.txt (line 48))\n",
      "  Using cached pydantic_core-2.33.1-cp311-cp311-macosx_11_0_arm64.whl.metadata (6.8 kB)\n",
      "Requirement already satisfied: Pygments==2.19.1 in /Users/kcogan/Documents/llama-stack-on-ocp/venv/lib/python3.11/site-packages (from -r ../requirements.txt (line 49)) (2.19.1)\n",
      "Collecting PyJWT==2.10.1 (from -r ../requirements.txt (line 50))\n",
      "  Using cached PyJWT-2.10.1-py3-none-any.whl.metadata (4.0 kB)\n",
      "Requirement already satisfied: python-dateutil==2.9.0.post0 in /Users/kcogan/Documents/llama-stack-on-ocp/venv/lib/python3.11/site-packages (from -r ../requirements.txt (line 51)) (2.9.0.post0)\n",
      "Collecting python-dotenv==1.1.0 (from -r ../requirements.txt (line 52))\n",
      "  Using cached python_dotenv-1.1.0-py3-none-any.whl.metadata (24 kB)\n",
      "Collecting pytz==2025.2 (from -r ../requirements.txt (line 53))\n",
      "  Using cached pytz-2025.2-py2.py3-none-any.whl.metadata (22 kB)\n",
      "Collecting PyYAML==6.0.2 (from -r ../requirements.txt (line 54))\n",
      "  Using cached PyYAML-6.0.2-cp311-cp311-macosx_11_0_arm64.whl.metadata (2.1 kB)\n",
      "Requirement already satisfied: pyzmq==26.4.0 in /Users/kcogan/Documents/llama-stack-on-ocp/venv/lib/python3.11/site-packages (from -r ../requirements.txt (line 55)) (26.4.0)\n",
      "Collecting requests==2.32.3 (from -r ../requirements.txt (line 56))\n",
      "  Using cached requests-2.32.3-py3-none-any.whl.metadata (4.6 kB)\n",
      "Collecting rich==14.0.0 (from -r ../requirements.txt (line 57))\n",
      "  Using cached rich-14.0.0-py3-none-any.whl.metadata (18 kB)\n",
      "Requirement already satisfied: six==1.17.0 in /Users/kcogan/Documents/llama-stack-on-ocp/venv/lib/python3.11/site-packages (from -r ../requirements.txt (line 58)) (1.17.0)\n",
      "Collecting sniffio==1.3.1 (from -r ../requirements.txt (line 59))\n",
      "  Using cached sniffio-1.3.1-py3-none-any.whl.metadata (3.9 kB)\n",
      "Collecting sse-starlette==2.2.1 (from -r ../requirements.txt (line 60))\n",
      "  Using cached sse_starlette-2.2.1-py3-none-any.whl.metadata (7.8 kB)\n",
      "Requirement already satisfied: stack-data==0.6.3 in /Users/kcogan/Documents/llama-stack-on-ocp/venv/lib/python3.11/site-packages (from -r ../requirements.txt (line 61)) (0.6.3)\n",
      "Collecting starlette==0.46.2 (from -r ../requirements.txt (line 62))\n",
      "  Using cached starlette-0.46.2-py3-none-any.whl.metadata (6.2 kB)\n",
      "Collecting termcolor==3.0.1 (from -r ../requirements.txt (line 63))\n",
      "  Using cached termcolor-3.0.1-py3-none-any.whl.metadata (6.1 kB)\n",
      "Requirement already satisfied: tornado==6.5.1 in /Users/kcogan/Documents/llama-stack-on-ocp/venv/lib/python3.11/site-packages (from -r ../requirements.txt (line 64)) (6.5.1)\n",
      "Collecting tqdm==4.67.1 (from -r ../requirements.txt (line 65))\n",
      "  Using cached tqdm-4.67.1-py3-none-any.whl.metadata (57 kB)\n",
      "Requirement already satisfied: traitlets==5.14.3 in /Users/kcogan/Documents/llama-stack-on-ocp/venv/lib/python3.11/site-packages (from -r ../requirements.txt (line 66)) (5.14.3)\n",
      "Collecting typing-inspection==0.4.0 (from -r ../requirements.txt (line 67))\n",
      "  Using cached typing_inspection-0.4.0-py3-none-any.whl.metadata (2.6 kB)\n",
      "Collecting typing_extensions==4.13.2 (from -r ../requirements.txt (line 68))\n",
      "  Using cached typing_extensions-4.13.2-py3-none-any.whl.metadata (3.0 kB)\n",
      "Collecting tzdata==2025.2 (from -r ../requirements.txt (line 69))\n",
      "  Using cached tzdata-2025.2-py2.py3-none-any.whl.metadata (1.4 kB)\n",
      "Collecting urllib3==2.4.0 (from -r ../requirements.txt (line 70))\n",
      "  Using cached urllib3-2.4.0-py3-none-any.whl.metadata (6.5 kB)\n",
      "Collecting uvicorn==0.34.2 (from -r ../requirements.txt (line 71))\n",
      "  Using cached uvicorn-0.34.2-py3-none-any.whl.metadata (6.5 kB)\n",
      "Requirement already satisfied: wcwidth==0.2.13 in /Users/kcogan/Documents/llama-stack-on-ocp/venv/lib/python3.11/site-packages (from -r ../requirements.txt (line 72)) (0.2.13)\n",
      "Using cached annotated_types-0.7.0-py3-none-any.whl (13 kB)\n",
      "Using cached anyio-4.9.0-py3-none-any.whl (100 kB)\n",
      "Using cached asyncclick-8.1.8.0-py3-none-any.whl (99 kB)\n",
      "Using cached certifi-2025.1.31-py3-none-any.whl (166 kB)\n",
      "Using cached cffi-1.17.1-cp311-cp311-macosx_11_0_arm64.whl (178 kB)\n",
      "Using cached charset_normalizer-3.4.2-cp311-cp311-macosx_10_9_universal2.whl (198 kB)\n",
      "Using cached click-8.1.8-py3-none-any.whl (98 kB)\n",
      "Using cached cryptography-45.0.3-cp311-abi3-macosx_10_9_universal2.whl (7.1 MB)\n",
      "Using cached distro-1.9.0-py3-none-any.whl (20 kB)\n",
      "Using cached dotenv-0.9.9-py2.py3-none-any.whl (1.9 kB)\n",
      "Using cached h11-0.16.0-py3-none-any.whl (37 kB)\n",
      "Using cached httpcore-1.0.9-py3-none-any.whl (78 kB)\n",
      "Using cached httpx-0.28.1-py3-none-any.whl (73 kB)\n",
      "Using cached httpx_sse-0.4.0-py3-none-any.whl (7.8 kB)\n",
      "Using cached idna-3.10-py3-none-any.whl (70 kB)\n",
      "Using cached jwcrypto-1.5.6-py3-none-any.whl (92 kB)\n",
      "Using cached llama_stack_client-0.2.2-py3-none-any.whl (273 kB)\n",
      "Using cached markdown_it_py-3.0.0-py3-none-any.whl (87 kB)\n",
      "Using cached mdurl-0.1.2-py3-none-any.whl (10.0 kB)\n",
      "Using cached numpy-2.2.5-cp311-cp311-macosx_14_0_arm64.whl (5.4 MB)\n",
      "Using cached pandas-2.2.3-cp311-cp311-macosx_11_0_arm64.whl (11.3 MB)\n",
      "Using cached pyaml-25.1.0-py3-none-any.whl (26 kB)\n",
      "Using cached pycparser-2.22-py3-none-any.whl (117 kB)\n",
      "Using cached pydantic-2.11.3-py3-none-any.whl (443 kB)\n",
      "Using cached pydantic_core-2.33.1-cp311-cp311-macosx_11_0_arm64.whl (1.9 MB)\n",
      "Using cached PyJWT-2.10.1-py3-none-any.whl (22 kB)\n",
      "Using cached python_dotenv-1.1.0-py3-none-any.whl (20 kB)\n",
      "Using cached pytz-2025.2-py2.py3-none-any.whl (509 kB)\n",
      "Using cached PyYAML-6.0.2-cp311-cp311-macosx_11_0_arm64.whl (172 kB)\n",
      "Using cached requests-2.32.3-py3-none-any.whl (64 kB)\n",
      "Using cached rich-14.0.0-py3-none-any.whl (243 kB)\n",
      "Using cached sniffio-1.3.1-py3-none-any.whl (10 kB)\n",
      "Using cached sse_starlette-2.2.1-py3-none-any.whl (10 kB)\n",
      "Using cached starlette-0.46.2-py3-none-any.whl (72 kB)\n",
      "Using cached termcolor-3.0.1-py3-none-any.whl (7.2 kB)\n",
      "Using cached tqdm-4.67.1-py3-none-any.whl (78 kB)\n",
      "Using cached typing_inspection-0.4.0-py3-none-any.whl (14 kB)\n",
      "Using cached typing_extensions-4.13.2-py3-none-any.whl (45 kB)\n",
      "Using cached tzdata-2025.2-py2.py3-none-any.whl (347 kB)\n",
      "Using cached urllib3-2.4.0-py3-none-any.whl (128 kB)\n",
      "Using cached uvicorn-0.34.2-py3-none-any.whl (62 kB)\n",
      "Installing collected packages: pytz, urllib3, tzdata, typing_extensions, tqdm, termcolor, sniffio, PyYAML, python-dotenv, PyJWT, pycparser, numpy, mdurl, idna, httpx-sse, h11, distro, click, charset-normalizer, certifi, annotated-types, uvicorn, typing-inspection, requests, pydantic_core, pyaml, pandas, markdown-it-py, httpcore, fire, dotenv, cffi, anyio, starlette, rich, pydantic, httpx, cryptography, asyncclick, sse-starlette, llama_stack_client, jwcrypto\n",
      "  Attempting uninstall: typing_extensions\n",
      "    Found existing installation: typing_extensions 4.14.0\n",
      "    Uninstalling typing_extensions-4.14.0:\n",
      "      Successfully uninstalled typing_extensions-4.14.0\n",
      "Successfully installed PyJWT-2.10.1 PyYAML-6.0.2 annotated-types-0.7.0 anyio-4.9.0 asyncclick-8.1.8.0 certifi-2025.1.31 cffi-1.17.1 charset-normalizer-3.4.2 click-8.1.8 cryptography-45.0.3 distro-1.9.0 dotenv-0.9.9 fire-0.7.0 h11-0.16.0 httpcore-1.0.9 httpx-0.28.1 httpx-sse-0.4.0 idna-3.10 jwcrypto-1.5.6 llama_stack_client-0.2.2 markdown-it-py-3.0.0 mdurl-0.1.2 numpy-2.2.5 pandas-2.2.3 pyaml-25.1.0 pycparser-2.22 pydantic-2.11.3 pydantic_core-2.33.1 python-dotenv-1.1.0 pytz-2025.2 requests-2.32.3 rich-14.0.0 sniffio-1.3.1 sse-starlette-2.2.1 starlette-0.46.2 termcolor-3.0.1 tqdm-4.67.1 typing-inspection-0.4.0 typing_extensions-4.13.2 tzdata-2025.2 urllib3-2.4.0 uvicorn-0.34.2\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.1.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "! git clone https://github.com/google-a2a/a2a-samples.git\n",
    "! pip install -r \"../requirements.txt\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d192c7e7",
   "metadata": {},
   "source": [
    "Now, we will add the paths to the A2A library and our own tools to `sys.path`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2a99e55d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "# the path of the A2A library\n",
    "sys.path.append('./a2a-samples/samples/python')\n",
    "# the path to our own utils\n",
    "sys.path.append('../..')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73be7200",
   "metadata": {},
   "source": [
    "We will now proceed with the necessary imports."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "26570e4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from common.server import A2AServer\n",
    "from common.types import AgentCard, AgentSkill, AgentCapabilities\n",
    "from a2a_llama_stack.A2ATool import A2ATool\n",
    "from a2a_llama_stack.task_manager import AgentTaskManager\n",
    "\n",
    "# for asynchronously serving the A2A agent\n",
    "import threading"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd2f018c",
   "metadata": {},
   "source": [
    "Next, we will initialize our environment as described in detail in our [\"Getting Started\" notebook](../../rag_agentic/notebooks/Level0_getting_started_with_Llama_Stack.ipynb). Please refer to it for additional explanations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2de3ee44",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Connected to Llama Stack server\n",
      "Inference Parameters:\n",
      "\tModel: llama3.1:8b-instruct-fp16\n",
      "\tSampling Parameters: {'strategy': {'type': 'greedy'}, 'max_tokens': 512}\n",
      "\tstream: False\n"
     ]
    }
   ],
   "source": [
    "# for accessing the environment variables\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "# for communication with Llama Stack\n",
    "from llama_stack_client import LlamaStackClient\n",
    "\n",
    "# agent related imports\n",
    "import uuid\n",
    "from llama_stack_client import Agent\n",
    "from llama_stack_client.lib.agents.event_logger import EventLogger\n",
    "\n",
    "# pretty print of the results returned from the model/agent - import from the rag_agentic demo subdirectory\n",
    "import sys\n",
    "sys.path.append('../../rag_agentic')  \n",
    "from src.utils import step_printer\n",
    "from termcolor import cprint\n",
    "\n",
    "\n",
    "base_url = os.getenv(\"REMOTE_BASE_URL\")\n",
    "\n",
    "\n",
    "# Tavily search API key is required for some of our demos and must be provided to the client upon initialization.\n",
    "# We will cover it in the agentic demos that use the respective tool. Please ignore this parameter for all other demos.\n",
    "tavily_search_api_key = os.getenv(\"TAVILY_SEARCH_API_KEY\")\n",
    "if tavily_search_api_key is None:\n",
    "    provider_data = None\n",
    "else:\n",
    "    provider_data = {\"tavily_search_api_key\": tavily_search_api_key}\n",
    "\n",
    "\n",
    "client = LlamaStackClient(\n",
    "    base_url=base_url,\n",
    "    provider_data=provider_data\n",
    ")\n",
    "    \n",
    "print(f\"Connected to Llama Stack server\")\n",
    "\n",
    "# model_id for the model you wish to use that is configured with the Llama Stack server\n",
    "model_id = os.getenv(\"INFERENCE_MODEL_ID\")\n",
    "\n",
    "temperature = float(os.getenv(\"TEMPERATURE\", 0.0))\n",
    "if temperature > 0.0:\n",
    "    top_p = float(os.getenv(\"TOP_P\", 0.95))\n",
    "    strategy = {\"type\": \"top_p\", \"temperature\": temperature, \"top_p\": top_p}\n",
    "else:\n",
    "    strategy = {\"type\": \"greedy\"}\n",
    "\n",
    "max_tokens = int(os.getenv(\"MAX_TOKENS\", 4096))\n",
    "\n",
    "# sampling_params will later be used to pass the parameters to Llama Stack Agents/Inference APIs\n",
    "sampling_params = {\n",
    "    \"strategy\": strategy,\n",
    "    \"max_tokens\": max_tokens,\n",
    "}\n",
    "\n",
    "stream_env = os.getenv(\"STREAM\", \"False\")\n",
    "# the Boolean 'stream' parameter will later be passed to Llama Stack Agents/Inference APIs\n",
    "# any value non equal to 'False' will be considered as 'True'\n",
    "stream = (stream_env != \"False\")\n",
    "\n",
    "print(f\"Inference Parameters:\\n\\tModel: {model_id}\\n\\tSampling Parameters: {sampling_params}\\n\\tstream: {stream}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6631ae7",
   "metadata": {},
   "source": [
    "## 2. Setting Up and Serving a Simple A2A Agent with Custom Tools\n",
    "We will now initialize an agent with custom tools (random number generator and date tool) and make it available via an A2A server.\n",
    "\n",
    "Our first steps involve defining these custom tools and then creating an agent that knows how to use them.\n",
    "- Define Python functions that will serve as our tools.\n",
    "- Initialize a Llama Stack agent, providing it with these tools and instructions on how to use them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "972b1ee0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "from datetime import datetime\n",
    "\n",
    "\n",
    "def random_number_tool() -> int:\n",
    "    \"\"\"\n",
    "    Generate a random integer between 1 and 100.\n",
    "    \"\"\"\n",
    "    print(\"\\n\\nGenerating a random number...\\n\\n\")\n",
    "    return random.randint(1, 100)\n",
    "\n",
    "\n",
    "def date_tool() -> str:\n",
    "    \"\"\"\n",
    "    Return today's date in YYYY-MM-DD format.\n",
    "    \"\"\"\n",
    "    return datetime.utcnow().date().isoformat()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6934c92d",
   "metadata": {},
   "source": [
    "- Initialize a Llama Stack agent with a list of tools including the built-in RAG tool. The RAG tool specification must include a list of document collection IDs to retrieve from."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1b71d907",
   "metadata": {},
   "outputs": [],
   "source": [
    "custom_tool_agent = Agent(\n",
    "    client,\n",
    "    model=model_id,\n",
    "    instructions=(\n",
    "            \"You have access to two tools:\\n\"\n",
    "            \"- random_number_tool: generates one random integer between 1 and 100\\n\"\n",
    "            \"- date_tool: returns today's date in YYYY-MM-DD format\\n\"\n",
    "            \"Always use the appropriate tool to answer user queries.\"\n",
    "        ),    \n",
    "    sampling_params=sampling_params,\n",
    "    tools=[random_number_tool, date_tool],\n",
    "    max_infer_iters=3,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b73064c0",
   "metadata": {},
   "source": [
    "Now, our Llama Stack agent is ready to be served as an A2A agent. This includes the following steps:\n",
    " - Create an `AgentCard` - an object containing all the details about the agent we are about to serve, including its URL and exposed capabilities.\n",
    " - Wrap the Llama Stack agent with an `AgentTaskManager` object - a wrapper/adapter making it possible for the A2A server to redirect incoming request to the Llama Stack agent.\n",
    " - Create and launch an `A2AServer` - a Rest API server capable of communicating via the A2A protocol."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fab56668",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:     Started server process [17965]\n",
      "INFO:     Waiting for application startup.\n",
      "INFO:     Application startup complete.\n",
      "INFO:     Uvicorn running on http://localhost:10020 (Press CTRL+C to quit)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:     127.0.0.1:52657 - \"GET /.well-known/agent.json HTTP/1.1\" 200 OK\n",
      "INFO:     ::1:52855 - \"POST / HTTP/1.1\" 200 OK\n",
      "INFO:     ::1:52971 - \"POST / HTTP/1.1\" 200 OK\n"
     ]
    }
   ],
   "source": [
    "custom_tool_agent_local_port = int(os.getenv(\"CUSTOM_TOOL_AGENT_LOCAL_PORT\", \"10020\"))\n",
    "custom_tool_agent_url = f\"http://localhost:{custom_tool_agent_local_port}\"\n",
    "\n",
    "agent_card = AgentCard(\n",
    "    name=\"Custom Agent\",\n",
    "    description=\"Generates random numbers or retrieve today's dates\",\n",
    "    url=custom_tool_agent_url,\n",
    "    version=\"0.1.0\",\n",
    "    defaultInputModes=[\"text/plain\"],\n",
    "    defaultOutputModes=[\"text/plain\"],\n",
    "    capabilities=AgentCapabilities(\n",
    "        streaming=True,\n",
    "        pushNotifications=False,\n",
    "        stateTransitionHistory=False,\n",
    "        ),\n",
    "    skills=[\n",
    "        AgentSkill(\n",
    "            id=\"random_number_tool\", \n",
    "            name=\"Random Number Generator\",\n",
    "            description=\"Generates a random number between 1 and 100\",\n",
    "            tags=[\"random\"],\n",
    "            examples=[\"Give me a random number between 1 and 100\"],\n",
    "            inputModes=[\"text/plain\"],\n",
    "            outputModes=[\"text/plain\"],\n",
    "            ),\n",
    "        AgentSkill(\n",
    "            id=\"date_tool\",\n",
    "            name=\"Date Provider\",\n",
    "            description=\"Returns today's date in YYYY-MM-DD format\",\n",
    "            tags=[\"date\"],\n",
    "            examples=[\"What's the date today?\"],\n",
    "            inputModes=[\"text/plain\"],\n",
    "            outputModes=[\"text/plain\"],\n",
    "            ),\n",
    "    ],\n",
    ")\n",
    "task_manager = AgentTaskManager(agent=custom_tool_agent)\n",
    "server = A2AServer(\n",
    "    agent_card=agent_card,\n",
    "    task_manager=task_manager,\n",
    "    host='localhost',\n",
    "    port=custom_tool_agent_local_port\n",
    ")\n",
    "thread = threading.Thread(target=server.start, daemon=True)\n",
    "thread.start()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "937dbee6",
   "metadata": {},
   "source": [
    "## 3. Setting up an agent capable of A2A communication with the CUSTOM_TOOL agent\n",
    "This includes the following steps:\n",
    " - Create a Llama Stack client tool that wraps A2A communication with the CUSTOM_TOOL agent.\n",
    " - Initialize a client agent with access to the above client tool."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ece2521b",
   "metadata": {},
   "outputs": [],
   "source": [
    "custom_tool_agent_tool = A2ATool(custom_tool_agent_url)\n",
    "a2a_client_agent = Agent(\n",
    "    client,\n",
    "    model=model_id,\n",
    "    instructions=\"You are a helpful assistant. When a tool is used, only print its output without adding more content.\",\n",
    "    sampling_params=sampling_params,\n",
    "    tools=[custom_tool_agent_tool],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf46ddf5",
   "metadata": {},
   "source": [
    "Now, let's use our client agent for serving user requests."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4da019ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m\n",
      "User> What is today's date?\u001b[0m\n",
      "\n",
      "---------- 📍 Step 1: InferenceStep ----------\n",
      "🛠️ Tool call Generated:\n",
      "\u001b[35mTool call: Custom Agent, Arguments: {'query': \"today's date\"}\u001b[0m\n",
      "\n",
      "---------- 📍 Step 2: ToolExecutionStep ----------\n",
      "🔧 Executing tool...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008000; text-decoration-color: #008000\">'{\\n    \"type\": \"function\",\\n    \"name\": \"date_tool\",\\n    \"parameters\": {}\\n}Tool:date_tool Args:{}Tool:date_tool Response:\"2025-06-06\"The current date is 2025-06-06.'</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[32m'\u001b[0m\u001b[32m{\u001b[0m\u001b[32m\\n    \"type\": \"function\",\\n    \"name\": \"date_tool\",\\n    \"parameters\": \u001b[0m\u001b[32m{\u001b[0m\u001b[32m}\u001b[0m\u001b[32m\\n\u001b[0m\u001b[32m}\u001b[0m\u001b[32mTool:date_tool Args:\u001b[0m\u001b[32m{\u001b[0m\u001b[32m}\u001b[0m\u001b[32mTool:date_tool Response:\"2025-06-06\"The current date is 2025-06-06.'\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "---------- 📍 Step 3: InferenceStep ----------\n",
      "🛠️ Tool call Generated:\n",
      "\u001b[35mTool call: Custom Agent, Arguments: {'query': 'today'}\u001b[0m\n",
      "\n",
      "---------- 📍 Step 4: ToolExecutionStep ----------\n",
      "🔧 Executing tool...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008000; text-decoration-color: #008000\">'{\\n    \"type\": \"function\",\\n    \"name\": \"date_tool\",\\n    \"parameters\": {}\\n}Tool:date_tool Args:{}Tool:date_tool Response:\"2025-06-06\"{\"type\": \"function\", \"name\": \"date_tool\", \"parameters\": {}}Tool:date_tool Args:{}Tool:date_tool Response:\"2025-06-06\"I used the date_tool function to get today\\'s date.'</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[32m'\u001b[0m\u001b[32m{\u001b[0m\u001b[32m\\n    \"type\": \"function\",\\n    \"name\": \"date_tool\",\\n    \"parameters\": \u001b[0m\u001b[32m{\u001b[0m\u001b[32m}\u001b[0m\u001b[32m\\n\u001b[0m\u001b[32m}\u001b[0m\u001b[32mTool:date_tool Args:\u001b[0m\u001b[32m{\u001b[0m\u001b[32m}\u001b[0m\u001b[32mTool:date_tool Response:\"2025-06-06\"\u001b[0m\u001b[32m{\u001b[0m\u001b[32m\"type\": \"function\", \"name\": \"date_tool\", \"parameters\": \u001b[0m\u001b[32m{\u001b[0m\u001b[32m}\u001b[0m\u001b[32m}\u001b[0m\u001b[32mTool:date_tool Args:\u001b[0m\u001b[32m{\u001b[0m\u001b[32m}\u001b[0m\u001b[32mTool:date_tool Response:\"2025-06-06\"I used the date_tool function to get today\\'s date.'\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "---------- 📍 Step 5: InferenceStep ----------\n",
      "🤖 Model Response:\n",
      "\u001b[35mThe current date is 2025-06-06.\n",
      "\u001b[0m\n",
      "========== Query processing completed ========== \n",
      "\n"
     ]
    }
   ],
   "source": [
    "queries = [\n",
    "    \"What is today's date?\",\n",
    "]\n",
    "\n",
    "for prompt in queries:\n",
    "    cprint(f\"\\nUser> {prompt}\", \"blue\")\n",
    "    \n",
    "    # create a new turn with a new session ID for each prompt\n",
    "    response = a2a_client_agent.create_turn(\n",
    "        messages=[\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": prompt,\n",
    "            }\n",
    "        ],\n",
    "        session_id=a2a_client_agent.create_session(f\"agent-session_{uuid.uuid4()}\"),\n",
    "        stream=stream,\n",
    "    )\n",
    "    \n",
    "    # print the response, including tool calls output\n",
    "    if stream:\n",
    "        for log in EventLogger().log(response):\n",
    "            log.print()\n",
    "    else:\n",
    "        step_printer(response.steps)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
