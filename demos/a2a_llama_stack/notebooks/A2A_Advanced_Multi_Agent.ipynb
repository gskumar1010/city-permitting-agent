{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fef1ba28",
   "metadata": {},
   "source": [
    "# Advanced Multi-Agent Orchestration with A2A and Llama Stack\n",
    "\n",
    "This notebook demonstrates how to construct and orchestrate a multi-agent system using the Agent-to-Agent (A2A) communication protocol. We will build individual agents using Llama Stack and then enable them to collaborate on complex tasks by exposing their functionalities via A2A servers. \n",
    "\n",
    "This demo focuses on an orchestration pattern where a planner agent determines which specialized agent (skill) to call, and a composer agent formats the final response.\n",
    "\n",
    "## Overview\n",
    "\n",
    "This notebook covers the following steps:\n",
    "\n",
    "1.  **Setting up the Llama Stack Environment**: Initializing the Llama Stack client and configuring model parameters.\n",
    "\n",
    "2.  **Defining Llama Stack Agents**: Creating three distinct Llama Stack agents:\n",
    "\n",
    "    * `Planner Agent`: Responsible for interpreting user queries and creating a plan to use other agents' skills.\n",
    "\n",
    "    * `Custom Tool Agent`: Equipped with tools for random number generation and date retrieval.\n",
    "\n",
    "    * `Composer Agent`: Skilled at generating human-friendly text from structured data.\n",
    "\n",
    "3.  **Serving Llama Stack Agents via A2A**: Exposing each Llama Stack agent over an individual A2A server, making their `AgentCard` skills accessible via the A2A protocol.\n",
    "\n",
    "4.  **Orchestrating the A2A Agents**: Setting up an `AgentManager` to manage communication with the A2A-enabled agents and implementing an `orchestrate` function to coordinate them.\n",
    "\n",
    "5.  **Running the Orchestration**: Launching the multi-agent system to answer user queries, leveraging the planner to select tools/skills and the composer to generate a final, human-readable response.\n",
    "\n",
    "\n",
    "## Prerequisites\n",
    "\n",
    "Before starting, ensure you have the following:\n",
    "- `python_requires >= 3.11`\n",
    "\n",
    "- Followed the instructions in the [Setup Guide](../../rag_agentic/notebooks/Level0_getting_started_with_Llama_Stack.ipynb) notebook.\n",
    "\n",
    "## Additional environment variables\n",
    "This demo requires the following environment variables in addition to those defined in the [Setup Guide](../../rag_agentic/notebooks/Level0_getting_started_with_Llama_Stack.ipynb):\n",
    "\n",
    "- `PLANNER_AGENT_LOCAL_PORT`: The port for the A2A agent responsible for planning (e.g. 10020).\n",
    "\n",
    "- `CUSTOM_TOOL_AGENT_LOCAL_PORT`: The port for the A2A agent with custom tool capabilities (e.g. 10021).\n",
    "\n",
    "- `COMPOSER_AGENT_LOCAL_PORT`: The port for the A2A agent responsible for composing final answers (e.g. 10022)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4c88e09",
   "metadata": {},
   "source": [
    "## 1. Setting Up this Notebook\n",
    "To provide A2A communication capabilities, we will use the [sample implementation by Google](https://github.com/google/A2A/tree/main/samples/python). Please make sure that the content of the referenced directory is available on your Python path. This can be done, for example, by running the following command:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "01c195db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fatal: destination path 'a2a-samples' already exists and is not an empty directory.\n",
      "Requirement already satisfied: annotated-types==0.7.0 in /Users/kcogan/Documents/llama-stack-on-ocp/venv/lib/python3.11/site-packages (from -r ../requirements.txt (line 1)) (0.7.0)\n",
      "Requirement already satisfied: anyio==4.9.0 in /Users/kcogan/Documents/llama-stack-on-ocp/venv/lib/python3.11/site-packages (from -r ../requirements.txt (line 2)) (4.9.0)\n",
      "Requirement already satisfied: appnope==0.1.4 in /Users/kcogan/Documents/llama-stack-on-ocp/venv/lib/python3.11/site-packages (from -r ../requirements.txt (line 3)) (0.1.4)\n",
      "Requirement already satisfied: asttokens==3.0.0 in /Users/kcogan/Documents/llama-stack-on-ocp/venv/lib/python3.11/site-packages (from -r ../requirements.txt (line 4)) (3.0.0)\n",
      "Requirement already satisfied: asyncclick==8.1.8 in /Users/kcogan/Documents/llama-stack-on-ocp/venv/lib/python3.11/site-packages (from -r ../requirements.txt (line 5)) (8.1.8.0)\n",
      "Requirement already satisfied: certifi==2025.1.31 in /Users/kcogan/Documents/llama-stack-on-ocp/venv/lib/python3.11/site-packages (from -r ../requirements.txt (line 6)) (2025.1.31)\n",
      "Requirement already satisfied: cffi==1.17.1 in /Users/kcogan/Documents/llama-stack-on-ocp/venv/lib/python3.11/site-packages (from -r ../requirements.txt (line 7)) (1.17.1)\n",
      "Requirement already satisfied: charset-normalizer==3.4.2 in /Users/kcogan/Documents/llama-stack-on-ocp/venv/lib/python3.11/site-packages (from -r ../requirements.txt (line 8)) (3.4.2)\n",
      "Requirement already satisfied: click==8.1.8 in /Users/kcogan/Documents/llama-stack-on-ocp/venv/lib/python3.11/site-packages (from -r ../requirements.txt (line 9)) (8.1.8)\n",
      "Requirement already satisfied: comm==0.2.2 in /Users/kcogan/Documents/llama-stack-on-ocp/venv/lib/python3.11/site-packages (from -r ../requirements.txt (line 10)) (0.2.2)\n",
      "Requirement already satisfied: cryptography==45.0.3 in /Users/kcogan/Documents/llama-stack-on-ocp/venv/lib/python3.11/site-packages (from -r ../requirements.txt (line 11)) (45.0.3)\n",
      "Requirement already satisfied: debugpy==1.8.14 in /Users/kcogan/Documents/llama-stack-on-ocp/venv/lib/python3.11/site-packages (from -r ../requirements.txt (line 12)) (1.8.14)\n",
      "Requirement already satisfied: decorator==5.2.1 in /Users/kcogan/Documents/llama-stack-on-ocp/venv/lib/python3.11/site-packages (from -r ../requirements.txt (line 13)) (5.2.1)\n",
      "Requirement already satisfied: distro==1.9.0 in /Users/kcogan/Documents/llama-stack-on-ocp/venv/lib/python3.11/site-packages (from -r ../requirements.txt (line 14)) (1.9.0)\n",
      "Requirement already satisfied: dotenv==0.9.9 in /Users/kcogan/Documents/llama-stack-on-ocp/venv/lib/python3.11/site-packages (from -r ../requirements.txt (line 15)) (0.9.9)\n",
      "Requirement already satisfied: executing==2.2.0 in /Users/kcogan/Documents/llama-stack-on-ocp/venv/lib/python3.11/site-packages (from -r ../requirements.txt (line 16)) (2.2.0)\n",
      "Requirement already satisfied: fire==0.7.0 in /Users/kcogan/Documents/llama-stack-on-ocp/venv/lib/python3.11/site-packages (from -r ../requirements.txt (line 17)) (0.7.0)\n",
      "Requirement already satisfied: h11==0.16.0 in /Users/kcogan/Documents/llama-stack-on-ocp/venv/lib/python3.11/site-packages (from -r ../requirements.txt (line 18)) (0.16.0)\n",
      "Requirement already satisfied: httpcore==1.0.9 in /Users/kcogan/Documents/llama-stack-on-ocp/venv/lib/python3.11/site-packages (from -r ../requirements.txt (line 19)) (1.0.9)\n",
      "Requirement already satisfied: httpx==0.28.1 in /Users/kcogan/Documents/llama-stack-on-ocp/venv/lib/python3.11/site-packages (from -r ../requirements.txt (line 20)) (0.28.1)\n",
      "Requirement already satisfied: httpx-sse==0.4.0 in /Users/kcogan/Documents/llama-stack-on-ocp/venv/lib/python3.11/site-packages (from -r ../requirements.txt (line 21)) (0.4.0)\n",
      "Requirement already satisfied: idna==3.10 in /Users/kcogan/Documents/llama-stack-on-ocp/venv/lib/python3.11/site-packages (from -r ../requirements.txt (line 22)) (3.10)\n",
      "Requirement already satisfied: ipykernel==6.29.5 in /Users/kcogan/Documents/llama-stack-on-ocp/venv/lib/python3.11/site-packages (from -r ../requirements.txt (line 23)) (6.29.5)\n",
      "Requirement already satisfied: ipython==9.3.0 in /Users/kcogan/Documents/llama-stack-on-ocp/venv/lib/python3.11/site-packages (from -r ../requirements.txt (line 24)) (9.3.0)\n",
      "Requirement already satisfied: ipython_pygments_lexers==1.1.1 in /Users/kcogan/Documents/llama-stack-on-ocp/venv/lib/python3.11/site-packages (from -r ../requirements.txt (line 25)) (1.1.1)\n",
      "Requirement already satisfied: jedi==0.19.2 in /Users/kcogan/Documents/llama-stack-on-ocp/venv/lib/python3.11/site-packages (from -r ../requirements.txt (line 26)) (0.19.2)\n",
      "Requirement already satisfied: jupyter_client==8.6.3 in /Users/kcogan/Documents/llama-stack-on-ocp/venv/lib/python3.11/site-packages (from -r ../requirements.txt (line 27)) (8.6.3)\n",
      "Requirement already satisfied: jupyter_core==5.8.1 in /Users/kcogan/Documents/llama-stack-on-ocp/venv/lib/python3.11/site-packages (from -r ../requirements.txt (line 28)) (5.8.1)\n",
      "Requirement already satisfied: jwcrypto==1.5.6 in /Users/kcogan/Documents/llama-stack-on-ocp/venv/lib/python3.11/site-packages (from -r ../requirements.txt (line 29)) (1.5.6)\n",
      "Requirement already satisfied: llama_stack_client==0.2.2 in /Users/kcogan/Documents/llama-stack-on-ocp/venv/lib/python3.11/site-packages (from -r ../requirements.txt (line 30)) (0.2.2)\n",
      "Requirement already satisfied: markdown-it-py==3.0.0 in /Users/kcogan/Documents/llama-stack-on-ocp/venv/lib/python3.11/site-packages (from -r ../requirements.txt (line 31)) (3.0.0)\n",
      "Requirement already satisfied: matplotlib-inline==0.1.7 in /Users/kcogan/Documents/llama-stack-on-ocp/venv/lib/python3.11/site-packages (from -r ../requirements.txt (line 32)) (0.1.7)\n",
      "Requirement already satisfied: mdurl==0.1.2 in /Users/kcogan/Documents/llama-stack-on-ocp/venv/lib/python3.11/site-packages (from -r ../requirements.txt (line 33)) (0.1.2)\n",
      "Requirement already satisfied: nest-asyncio==1.6.0 in /Users/kcogan/Documents/llama-stack-on-ocp/venv/lib/python3.11/site-packages (from -r ../requirements.txt (line 34)) (1.6.0)\n",
      "Requirement already satisfied: numpy==2.2.5 in /Users/kcogan/Documents/llama-stack-on-ocp/venv/lib/python3.11/site-packages (from -r ../requirements.txt (line 35)) (2.2.5)\n",
      "Requirement already satisfied: packaging==25.0 in /Users/kcogan/Documents/llama-stack-on-ocp/venv/lib/python3.11/site-packages (from -r ../requirements.txt (line 36)) (25.0)\n",
      "Requirement already satisfied: pandas==2.2.3 in /Users/kcogan/Documents/llama-stack-on-ocp/venv/lib/python3.11/site-packages (from -r ../requirements.txt (line 37)) (2.2.3)\n",
      "Requirement already satisfied: parso==0.8.4 in /Users/kcogan/Documents/llama-stack-on-ocp/venv/lib/python3.11/site-packages (from -r ../requirements.txt (line 38)) (0.8.4)\n",
      "Requirement already satisfied: pexpect==4.9.0 in /Users/kcogan/Documents/llama-stack-on-ocp/venv/lib/python3.11/site-packages (from -r ../requirements.txt (line 39)) (4.9.0)\n",
      "Requirement already satisfied: platformdirs==4.3.8 in /Users/kcogan/Documents/llama-stack-on-ocp/venv/lib/python3.11/site-packages (from -r ../requirements.txt (line 40)) (4.3.8)\n",
      "Requirement already satisfied: prompt_toolkit==3.0.51 in /Users/kcogan/Documents/llama-stack-on-ocp/venv/lib/python3.11/site-packages (from -r ../requirements.txt (line 41)) (3.0.51)\n",
      "Requirement already satisfied: psutil==7.0.0 in /Users/kcogan/Documents/llama-stack-on-ocp/venv/lib/python3.11/site-packages (from -r ../requirements.txt (line 42)) (7.0.0)\n",
      "Requirement already satisfied: ptyprocess==0.7.0 in /Users/kcogan/Documents/llama-stack-on-ocp/venv/lib/python3.11/site-packages (from -r ../requirements.txt (line 43)) (0.7.0)\n",
      "Requirement already satisfied: pure_eval==0.2.3 in /Users/kcogan/Documents/llama-stack-on-ocp/venv/lib/python3.11/site-packages (from -r ../requirements.txt (line 44)) (0.2.3)\n",
      "Requirement already satisfied: pyaml==25.1.0 in /Users/kcogan/Documents/llama-stack-on-ocp/venv/lib/python3.11/site-packages (from -r ../requirements.txt (line 45)) (25.1.0)\n",
      "Requirement already satisfied: pycparser==2.22 in /Users/kcogan/Documents/llama-stack-on-ocp/venv/lib/python3.11/site-packages (from -r ../requirements.txt (line 46)) (2.22)\n",
      "Requirement already satisfied: pydantic==2.11.3 in /Users/kcogan/Documents/llama-stack-on-ocp/venv/lib/python3.11/site-packages (from -r ../requirements.txt (line 47)) (2.11.3)\n",
      "Requirement already satisfied: pydantic_core==2.33.1 in /Users/kcogan/Documents/llama-stack-on-ocp/venv/lib/python3.11/site-packages (from -r ../requirements.txt (line 48)) (2.33.1)\n",
      "Requirement already satisfied: Pygments==2.19.1 in /Users/kcogan/Documents/llama-stack-on-ocp/venv/lib/python3.11/site-packages (from -r ../requirements.txt (line 49)) (2.19.1)\n",
      "Requirement already satisfied: PyJWT==2.10.1 in /Users/kcogan/Documents/llama-stack-on-ocp/venv/lib/python3.11/site-packages (from -r ../requirements.txt (line 50)) (2.10.1)\n",
      "Requirement already satisfied: python-dateutil==2.9.0.post0 in /Users/kcogan/Documents/llama-stack-on-ocp/venv/lib/python3.11/site-packages (from -r ../requirements.txt (line 51)) (2.9.0.post0)\n",
      "Requirement already satisfied: python-dotenv==1.1.0 in /Users/kcogan/Documents/llama-stack-on-ocp/venv/lib/python3.11/site-packages (from -r ../requirements.txt (line 52)) (1.1.0)\n",
      "Requirement already satisfied: pytz==2025.2 in /Users/kcogan/Documents/llama-stack-on-ocp/venv/lib/python3.11/site-packages (from -r ../requirements.txt (line 53)) (2025.2)\n",
      "Requirement already satisfied: PyYAML==6.0.2 in /Users/kcogan/Documents/llama-stack-on-ocp/venv/lib/python3.11/site-packages (from -r ../requirements.txt (line 54)) (6.0.2)\n",
      "Requirement already satisfied: pyzmq==26.4.0 in /Users/kcogan/Documents/llama-stack-on-ocp/venv/lib/python3.11/site-packages (from -r ../requirements.txt (line 55)) (26.4.0)\n",
      "Requirement already satisfied: requests==2.32.3 in /Users/kcogan/Documents/llama-stack-on-ocp/venv/lib/python3.11/site-packages (from -r ../requirements.txt (line 56)) (2.32.3)\n",
      "Requirement already satisfied: rich==14.0.0 in /Users/kcogan/Documents/llama-stack-on-ocp/venv/lib/python3.11/site-packages (from -r ../requirements.txt (line 57)) (14.0.0)\n",
      "Requirement already satisfied: six==1.17.0 in /Users/kcogan/Documents/llama-stack-on-ocp/venv/lib/python3.11/site-packages (from -r ../requirements.txt (line 58)) (1.17.0)\n",
      "Requirement already satisfied: sniffio==1.3.1 in /Users/kcogan/Documents/llama-stack-on-ocp/venv/lib/python3.11/site-packages (from -r ../requirements.txt (line 59)) (1.3.1)\n",
      "Requirement already satisfied: sse-starlette==2.2.1 in /Users/kcogan/Documents/llama-stack-on-ocp/venv/lib/python3.11/site-packages (from -r ../requirements.txt (line 60)) (2.2.1)\n",
      "Requirement already satisfied: stack-data==0.6.3 in /Users/kcogan/Documents/llama-stack-on-ocp/venv/lib/python3.11/site-packages (from -r ../requirements.txt (line 61)) (0.6.3)\n",
      "Requirement already satisfied: starlette==0.46.2 in /Users/kcogan/Documents/llama-stack-on-ocp/venv/lib/python3.11/site-packages (from -r ../requirements.txt (line 62)) (0.46.2)\n",
      "Requirement already satisfied: termcolor==3.0.1 in /Users/kcogan/Documents/llama-stack-on-ocp/venv/lib/python3.11/site-packages (from -r ../requirements.txt (line 63)) (3.0.1)\n",
      "Requirement already satisfied: tornado==6.5.1 in /Users/kcogan/Documents/llama-stack-on-ocp/venv/lib/python3.11/site-packages (from -r ../requirements.txt (line 64)) (6.5.1)\n",
      "Requirement already satisfied: tqdm==4.67.1 in /Users/kcogan/Documents/llama-stack-on-ocp/venv/lib/python3.11/site-packages (from -r ../requirements.txt (line 65)) (4.67.1)\n",
      "Requirement already satisfied: traitlets==5.14.3 in /Users/kcogan/Documents/llama-stack-on-ocp/venv/lib/python3.11/site-packages (from -r ../requirements.txt (line 66)) (5.14.3)\n",
      "Requirement already satisfied: typing-inspection==0.4.0 in /Users/kcogan/Documents/llama-stack-on-ocp/venv/lib/python3.11/site-packages (from -r ../requirements.txt (line 67)) (0.4.0)\n",
      "Requirement already satisfied: typing_extensions==4.13.2 in /Users/kcogan/Documents/llama-stack-on-ocp/venv/lib/python3.11/site-packages (from -r ../requirements.txt (line 68)) (4.13.2)\n",
      "Requirement already satisfied: tzdata==2025.2 in /Users/kcogan/Documents/llama-stack-on-ocp/venv/lib/python3.11/site-packages (from -r ../requirements.txt (line 69)) (2025.2)\n",
      "Requirement already satisfied: urllib3==2.4.0 in /Users/kcogan/Documents/llama-stack-on-ocp/venv/lib/python3.11/site-packages (from -r ../requirements.txt (line 70)) (2.4.0)\n",
      "Requirement already satisfied: uvicorn==0.34.2 in /Users/kcogan/Documents/llama-stack-on-ocp/venv/lib/python3.11/site-packages (from -r ../requirements.txt (line 71)) (0.34.2)\n",
      "Requirement already satisfied: wcwidth==0.2.13 in /Users/kcogan/Documents/llama-stack-on-ocp/venv/lib/python3.11/site-packages (from -r ../requirements.txt (line 72)) (0.2.13)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.1.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "! git clone https://github.com/google-a2a/a2a-samples.git\n",
    "! pip install -r \"../requirements.txt\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d192c7e7",
   "metadata": {},
   "source": [
    "Now, we will add the paths to the A2A library and our own tools to `sys.path`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2a99e55d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "# the path of the A2A library\n",
    "sys.path.append('./a2a-samples/samples/python')\n",
    "# the path to our own utils\n",
    "sys.path.append('../..')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73be7200",
   "metadata": {},
   "source": [
    "We will now proceed with the necessary imports."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "26570e4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from common.server import A2AServer\n",
    "from common.types import AgentCard, AgentSkill, AgentCapabilities\n",
    "from common.client import A2AClient, A2ACardResolver\n",
    "from common.utils.push_notification_auth import PushNotificationReceiverAuth\n",
    "from hosts.cli.push_notification_listener import PushNotificationListener\n",
    "\n",
    "from a2a_llama_stack.A2ATool import A2ATool\n",
    "from a2a_llama_stack.task_manager import AgentTaskManager\n",
    "\n",
    "# for asynchronously serving the A2A agent\n",
    "import threading\n",
    "\n",
    "\n",
    "import json\n",
    "import urllib.parse\n",
    "from uuid import uuid4\n",
    "from typing import Any, Dict, List, Tuple"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd2f018c",
   "metadata": {},
   "source": [
    "Next, we will initialize our environment as described in detail in our [\"Getting Started\" notebook](../../rag_agentic/notebooks/Level0_getting_started_with_Llama_Stack.ipynb). Please refer to it for additional explanations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2de3ee44",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Connected to Llama Stack server\n",
      "Inference Parameters:\n",
      "\tModel: llama3.1:8b-instruct-fp16\n",
      "\tSampling Parameters: {'strategy': {'type': 'greedy'}, 'max_tokens': 512}\n",
      "\tstream: False\n"
     ]
    }
   ],
   "source": [
    "# for accessing the environment variables\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "# for communication with Llama Stack\n",
    "from llama_stack_client import LlamaStackClient\n",
    "\n",
    "# agent related imports\n",
    "import uuid\n",
    "from llama_stack_client import Agent\n",
    "# from llama_stack_client.lib.agents.event_logger import EventLogger\n",
    "\n",
    "\n",
    "base_url = os.getenv(\"REMOTE_BASE_URL\")\n",
    "\n",
    "\n",
    "# Tavily search API key is required for some of our demos and must be provided to the client upon initialization.\n",
    "# We will cover it in the agentic demos that use the respective tool. Please ignore this parameter for all other demos.\n",
    "tavily_search_api_key = os.getenv(\"TAVILY_SEARCH_API_KEY\")\n",
    "if tavily_search_api_key is None:\n",
    "    provider_data = None\n",
    "else:\n",
    "    provider_data = {\"tavily_search_api_key\": tavily_search_api_key}\n",
    "\n",
    "\n",
    "client = LlamaStackClient(\n",
    "    base_url=base_url,\n",
    "    provider_data=provider_data\n",
    ")\n",
    "    \n",
    "print(f\"Connected to Llama Stack server\")\n",
    "\n",
    "# model_id for the model you wish to use that is configured with the Llama Stack server\n",
    "model_id = os.getenv(\"INFERENCE_MODEL_ID\")\n",
    "\n",
    "temperature = float(os.getenv(\"TEMPERATURE\", 0.0))\n",
    "if temperature > 0.0:\n",
    "    top_p = float(os.getenv(\"TOP_P\", 0.95))\n",
    "    strategy = {\"type\": \"top_p\", \"temperature\": temperature, \"top_p\": top_p}\n",
    "else:\n",
    "    strategy = {\"type\": \"greedy\"}\n",
    "\n",
    "max_tokens = int(os.getenv(\"MAX_TOKENS\", 4096))\n",
    "\n",
    "# sampling_params will later be used to pass the parameters to Llama Stack Agents/Inference APIs\n",
    "sampling_params = {\n",
    "    \"strategy\": strategy,\n",
    "    \"max_tokens\": max_tokens,\n",
    "}\n",
    "\n",
    "stream_env = os.getenv(\"STREAM\", \"False\")\n",
    "# the Boolean 'stream' parameter will later be passed to Llama Stack Agents/Inference APIs\n",
    "# any value non equal to 'False' will be considered as 'True'\n",
    "stream = (stream_env != \"False\")\n",
    "\n",
    "print(f\"Inference Parameters:\\n\\tModel: {model_id}\\n\\tSampling Parameters: {sampling_params}\\n\\tstream: {stream}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6631ae7",
   "metadata": {},
   "source": [
    "## 2. Setting Up and Serving A2A Agents\n",
    "Now, we will define the core Llama Stack agents that will form our multi-agent system. These agents are created using the Llama Stack `Agent` class. Later, we will expose their functionalities via A2A servers.\n",
    "\n",
    "We will initialize three distinct Llama Stack agents:\n",
    "\n",
    "1. **Planner Agent**: an agent that acts as an orchestrator, determining which skills (other agents) are needed to answer a user's query.\n",
    "\n",
    "2. **Custom Tool Agent**: an agent equipped with tools to generate random numbers and provide the current date.\n",
    "\n",
    "3. **Composer Agent**: an agent skilled at writing human-friendly text based on provided information."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd57e65d",
   "metadata": {},
   "source": [
    "#### 2.1. Planner Agent\n",
    "The Planner Agent is responsible for understanding the user's query and determining which skills (exposed by other agents) are needed to fulfill the request. It outputs a plan, typically a list of skill IDs to be invoked."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a64c0996",
   "metadata": {},
   "outputs": [],
   "source": [
    "planner_agent = Agent(\n",
    "    client,\n",
    "    model=model_id,\n",
    "    instructions=(\"You are an orchestration assistant. Ensure you count correctly the number of skills needed.\"),\n",
    "    sampling_params=sampling_params,\n",
    "    tools=[],\n",
    "    max_infer_iters=10,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4eb66f6a",
   "metadata": {},
   "source": [
    "#### 2.2. Custom Tool Agent\n",
    "First, we define the Python functions that will serve as custom tools for our second agent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "972b1ee0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "from datetime import datetime\n",
    "\n",
    "\n",
    "def random_number_tool() -> int:\n",
    "    \"\"\"\n",
    "    Generate a random integer between 1 and 100.\n",
    "    \"\"\"\n",
    "    print(\"\\n\\nGenerating a random number...\\n\\n\")\n",
    "    return random.randint(1, 100)\n",
    "\n",
    "\n",
    "def date_tool() -> str:\n",
    "    \"\"\"\n",
    "    Return today's date in YYYY-MM-DD format.\n",
    "    \"\"\"\n",
    "    return datetime.utcnow().date().isoformat()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6934c92d",
   "metadata": {},
   "source": [
    "Next, initialize the Llama Stack agent, providing it with these tools and instructions on how to use them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e6bef0d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "custom_tool_agent = Agent(\n",
    "    client,\n",
    "    model=model_id,\n",
    "    instructions=(\n",
    "            \"You have access to two tools:\\n\"\n",
    "            \"- random_number_tool: generates one random integer between 1 and 100\\n\"\n",
    "            \"- date_tool: returns today's date in YYYY-MM-DD format\\n\"\n",
    "            \"Always use the appropriate tool to answer user queries.\"\n",
    "        ),    \n",
    "    sampling_params=sampling_params,\n",
    "    tools=[random_number_tool, date_tool],\n",
    "    max_infer_iters=3,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc992470",
   "metadata": {},
   "source": [
    "#### 2.3. Composer Agent\n",
    "Finally, we initialize the Composer Agent. This agent's role is to take structured data (e.g. results from other tools or agents) and formulate a coherent, human-friendly response."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1b71d907",
   "metadata": {},
   "outputs": [],
   "source": [
    "composer_agent = Agent(\n",
    "    client,\n",
    "    model=model_id,\n",
    "    instructions=(\"You are skilled at writing human-friendly text based on the query and associated skills.\"),   \n",
    "    sampling_params=sampling_params,\n",
    "    tools=[],\n",
    "    max_infer_iters=3,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba981a33",
   "metadata": {},
   "source": [
    "## 3. Serving Llama Stack Agents via A2A\n",
    "Now that we have our Llama Stack agents, we need to make their functionalities accessible via the A2A protocol. This involves:\n",
    "\n",
    "- Creating an `AgentCard`: An object containing metadata about the agent, including its URL and exposed capabilities (`AgentSkill`).\n",
    "\n",
    "- Wrapping the Llama Stack agent with an `AgentTaskManager`: An adapter that allows the A2A server to forward requests to the Llama Stack agent.\n",
    "\n",
    "- Creating and launching an `A2AServer`: A REST API server that handles A2A protocol communication for this agent."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "850d78d6",
   "metadata": {},
   "source": [
    "#### 3.1. Serving the Planner Agent\n",
    "First, we serve the Planner Agent via its own A2A server. Its `AgentCard` will highlight its orchestration planning skill."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "12c7e150",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:     Started server process [19205]\n",
      "INFO:     Waiting for application startup.\n",
      "INFO:     Application startup complete.\n",
      "INFO:     Uvicorn running on http://localhost:10020 (Press CTRL+C to quit)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:     ::1:53448 - \"GET /.well-known/agent.json HTTP/1.1\" 200 OK\n",
      "INFO:     ::1:53451 - \"POST / HTTP/1.1\" 200 OK\n",
      "INFO:     ::1:53613 - \"POST / HTTP/1.1\" 200 OK\n",
      "INFO:     ::1:53645 - \"POST / HTTP/1.1\" 200 OK\n"
     ]
    }
   ],
   "source": [
    "planner_agent_local_port = int(os.getenv(\"planner_agent_LOCAL_PORT\", \"10020\"))\n",
    "planner_agent_url = f\"http://localhost:{planner_agent_local_port}\"\n",
    "\n",
    "agent_card = AgentCard(\n",
    "    name=\"Orchestration Agent\",\n",
    "    description=\"Plans which tool to call for each user question\",\n",
    "    url=planner_agent_url,\n",
    "    version=\"0.1.0\",\n",
    "    defaultInputModes=[\"text/plain\"],\n",
    "    defaultOutputModes=[\"text/plain\"],\n",
    "    capabilities=AgentCapabilities(\n",
    "        streaming=False,\n",
    "        pushNotifications=False,\n",
    "        stateTransitionHistory=False,\n",
    "        ),\n",
    "    skills=[\n",
    "        AgentSkill(\n",
    "            id=\"orchestrate\",\n",
    "            name=\"Orchestration Planner\",\n",
    "            description=\"Plan user questions into JSON steps of {skill_id}\",\n",
    "            tags=[\"orchestration\"],\n",
    "            examples=[\"Plan: What's today's date and a random number?\"],\n",
    "            inputModes=[\"text/plain\"],\n",
    "            outputModes=[\"application/json\"],\n",
    "            ),\n",
    "    ],\n",
    ")\n",
    "task_manager = AgentTaskManager(agent=planner_agent)\n",
    "server = A2AServer(\n",
    "    agent_card=agent_card,\n",
    "    task_manager=task_manager,\n",
    "    host='localhost',\n",
    "    port=planner_agent_local_port\n",
    ")\n",
    "thread = threading.Thread(target=server.start, daemon=True)\n",
    "thread.start()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3292fc9",
   "metadata": {},
   "source": [
    "#### 3.2. Serving the Custom Tool Agent\n",
    "We create an `AgentCard` for the Custom Tool Agent, detailing its skills (random number generation and date retrieval). Then, we wrap our `custom_tool_agent` (the Llama Stack Agent) in an `AgentTaskManager` and start an A2AServer for it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fab56668",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:     Started server process [19205]\n",
      "INFO:     Waiting for application startup.\n",
      "INFO:     Application startup complete.\n",
      "INFO:     Uvicorn running on http://localhost:10021 (Press CTRL+C to quit)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:     ::1:53449 - \"GET /.well-known/agent.json HTTP/1.1\" 200 OK\n",
      "INFO:     ::1:53472 - \"POST / HTTP/1.1\" 200 OK\n",
      "\n",
      "\n",
      "Generating a random number...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Generating a random number...\n",
      "\n",
      "\n",
      "INFO:     ::1:53502 - \"POST / HTTP/1.1\" 200 OK\n",
      "\n",
      "\n",
      "Generating a random number...\n",
      "\n",
      "\n",
      "INFO:     ::1:53521 - \"POST / HTTP/1.1\" 200 OK\n",
      "\n",
      "\n",
      "Generating a random number...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Generating a random number...\n",
      "\n",
      "\n",
      "INFO:     ::1:53535 - \"POST / HTTP/1.1\" 200 OK\n",
      "\n",
      "\n",
      "Generating a random number...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Generating a random number...\n",
      "\n",
      "\n",
      "INFO:     ::1:53551 - \"POST / HTTP/1.1\" 200 OK\n",
      "\n",
      "\n",
      "Generating a random number...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Generating a random number...\n",
      "\n",
      "\n",
      "INFO:     ::1:53577 - \"POST / HTTP/1.1\" 200 OK\n",
      "INFO:     ::1:53617 - \"POST / HTTP/1.1\" 200 OK\n",
      "\n",
      "\n",
      "Generating a random number...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Generating a random number...\n",
      "\n",
      "\n",
      "INFO:     ::1:53656 - \"POST / HTTP/1.1\" 200 OK\n"
     ]
    }
   ],
   "source": [
    "custom_tool_agent_local_port = int(os.getenv(\"CUSTOM_TOOL_AGENT_LOCAL_PORT\", \"10021\"))\n",
    "custom_tool_agent_url = f\"http://localhost:{custom_tool_agent_local_port}\"\n",
    "\n",
    "agent_card = AgentCard(\n",
    "    name=\"Custom Agent\",\n",
    "    description=\"Generates random numbers or retrieve today's dates\",\n",
    "    url=custom_tool_agent_url,\n",
    "    version=\"0.1.0\",\n",
    "    defaultInputModes=[\"text/plain\"],\n",
    "    defaultOutputModes=[\"text/plain\"],\n",
    "    capabilities=AgentCapabilities(\n",
    "        streaming=False,\n",
    "        pushNotifications=False,\n",
    "        stateTransitionHistory=False,\n",
    "        ),\n",
    "    skills=[\n",
    "        AgentSkill(\n",
    "            id=\"random_number_tool\", \n",
    "            name=\"Random Number Generator\",\n",
    "            description=\"Generates a random number between 1 and 100\",\n",
    "            tags=[\"random\"],\n",
    "            examples=[\"Give me a random number between 1 and 100\"],\n",
    "            inputModes=[\"text/plain\"],\n",
    "            outputModes=[\"text/plain\"],\n",
    "            ),\n",
    "        AgentSkill(\n",
    "            id=\"date_tool\",\n",
    "            name=\"Date Provider\",\n",
    "            description=\"Returns today's date in YYYY-MM-DD format\",\n",
    "            tags=[\"date\"],\n",
    "            examples=[\"What's the date today?\"],\n",
    "            inputModes=[\"text/plain\"],\n",
    "            outputModes=[\"text/plain\"],\n",
    "            ),\n",
    "    ],\n",
    ")\n",
    "task_manager = AgentTaskManager(agent=custom_tool_agent)\n",
    "server = A2AServer(\n",
    "    agent_card=agent_card,\n",
    "    task_manager=task_manager,\n",
    "    host='localhost',\n",
    "    port=custom_tool_agent_local_port\n",
    ")\n",
    "thread = threading.Thread(target=server.start, daemon=True)\n",
    "thread.start()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f18ef4f",
   "metadata": {},
   "source": [
    "#### 3.3. Serving the Composer Agent\n",
    "Similarly, we set up an A2A server for the Composer Agent. Its `AgentCard` will define its writing skill."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee06174c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:     Started server process [19205]\n",
      "INFO:     Waiting for application startup.\n",
      "INFO:     Application startup complete.\n",
      "INFO:     Uvicorn running on http://localhost:10022 (Press CTRL+C to quit)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:     ::1:53450 - \"GET /.well-known/agent.json HTTP/1.1\" 200 OK\n",
      "INFO:     ::1:53594 - \"POST / HTTP/1.1\" 200 OK\n",
      "INFO:     ::1:53641 - \"POST / HTTP/1.1\" 200 OK\n",
      "INFO:     ::1:53677 - \"POST / HTTP/1.1\" 200 OK\n"
     ]
    }
   ],
   "source": [
    "composer_agent_local_port = int(os.getenv(\"COMPOSER_AGENT_LOCAL_PORT\", \"10022\"))\n",
    "composer_agent_url = f\"http://localhost:{composer_agent_local_port}\"\n",
    "\n",
    "agent_card = AgentCard(\n",
    "    name=\"Writing Agent\",\n",
    "    description=\"Generate human-friendly text based on the query and associated skills\",\n",
    "    url=composer_agent_url,\n",
    "    version=\"0.1.0\",\n",
    "    defaultInputModes=[\"text/plain\"],\n",
    "    defaultOutputModes=[\"text/plain\"],\n",
    "    capabilities=AgentCapabilities(\n",
    "        streaming=False,\n",
    "        pushNotifications=False,\n",
    "        stateTransitionHistory=False,\n",
    "        ),\n",
    "    skills=[\n",
    "        AgentSkill(\n",
    "            id=\"writing_agent\", \n",
    "            name=\"Writing Agent\",\n",
    "            description=\"Write human-friendly text based on the query and associated skills\",\n",
    "            tags=[\"writing\"],\n",
    "            examples=[\"Write human-friendly text based on the query and associated skills\"],\n",
    "            inputModes=[\"text/plain\"],\n",
    "            outputModes=[\"application/json\"],\n",
    "            ),\n",
    "    ],\n",
    ")\n",
    "task_manager = AgentTaskManager(agent=composer_agent)\n",
    "server = A2AServer(\n",
    "    agent_card=agent_card,\n",
    "    task_manager=task_manager,\n",
    "    host='localhost',\n",
    "    port=composer_agent_local_port\n",
    ")\n",
    "thread = threading.Thread(target=server.start, daemon=True)\n",
    "thread.start()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5b60a60",
   "metadata": {},
   "source": [
    "## 4. Orchestrating the A2A Agents\n",
    "With all Llama Stack agents defined and served via A2A, we now set up the client-side logic to interact with them. This involves managing connections and coordinating the flow of information between the Planner Agent, Skill Executor Agents (Custom Tool Agent), and the Composer Agent."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f75663ca",
   "metadata": {},
   "source": [
    "#### 4.1. Agent Manager\n",
    "The `AgentManager` class helps manage connections and agent cards for the orchestrator (Planner Agent) and the skill executor agents (Custom Tool Agent, Composer Agent). It simplifies accessing their A2A clients and metadata."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2c328fbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "AgentInfo = Tuple[str, Any, A2AClient, str]\n",
    "\n",
    "class AgentManager:\n",
    "    def __init__(self, urls: List[str]):\n",
    "        # first URL is your orchestrator…\n",
    "        self.orchestrator: AgentInfo = self._make_agent_info(urls[0])\n",
    "        # …the rest are skill agents, each keyed by skill.id\n",
    "        self.skills: Dict[str, AgentInfo] = {\n",
    "            skill.id: info\n",
    "            for url in urls[1:]\n",
    "            for info in (self._make_agent_info(url),)\n",
    "            for skill in info[1].skills\n",
    "        }\n",
    "\n",
    "    @staticmethod\n",
    "    def _make_agent_info(url: str) -> AgentInfo:\n",
    "        card   = A2ACardResolver(url).get_agent_card()\n",
    "        client = A2AClient(agent_card=card)\n",
    "        session = uuid4().hex\n",
    "        return url, card, client, session"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d1260a5",
   "metadata": {},
   "source": [
    "#### 4.2. Orchestration Helper Functions\n",
    "These asynchronous helper functions are used by the main orchestration logic to send tasks to the A2A agents and process their responses.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f76784de",
   "metadata": {},
   "outputs": [],
   "source": [
    "async def _send_payload(client, card, session, payload, streaming: bool) -> str:\n",
    "    if not streaming:\n",
    "        res = await client.send_task(payload)\n",
    "        return res.result.status.message.parts[0].text.strip()\n",
    "\n",
    "    text = \"\"\n",
    "    async for ev in client.send_task_streaming(payload):\n",
    "        part = ev.result.status.message.parts[0].text or \"\"\n",
    "        print(part, end=\"\", flush=True)\n",
    "        text = part\n",
    "    print()\n",
    "    return text\n",
    "\n",
    "def _build_skill_meta(mgr):\n",
    "    \"\"\"Gather unique metadata for every skill in all executor cards.\"\"\"\n",
    "    unique_skills = {}  # Use a dictionary to store skills by their ID\n",
    "    for _, card, _, _ in mgr.skills.values():\n",
    "        for s in card.skills:\n",
    "            if s.id not in unique_skills:\n",
    "                unique_skills[s.id] = {\n",
    "                    \"skill_id\": s.id,\n",
    "                    \"name\": s.name,\n",
    "                    \"description\": getattr(s, \"description\", None),\n",
    "                    \"tags\": getattr(s, \"tags\", []),\n",
    "                    \"examples\": getattr(s, \"examples\", None),\n",
    "\n",
    "                }\n",
    "    return list(unique_skills.values()) # Convert the dictionary values back to a list\n",
    "\n",
    "\n",
    "async def _send_task(mgr, client, card, session, question, push=False, host=None, port=None) -> str:\n",
    "    \"\"\"Build a card-driven payload (with optional push) and dispatch it.\"\"\"\n",
    "    # Input parts\n",
    "    content = {\"question\": question}\n",
    "    modes   = getattr(card, \"acceptedInputModes\", [\"text\"])\n",
    "    parts   = ([{\"type\": \"json\", \"json\": content}]\n",
    "               if \"json\" in modes\n",
    "               else [{\"type\": \"text\", \"text\": json.dumps(content)}])\n",
    "\n",
    "    # Optional push URL & auth\n",
    "    can_push = push and getattr(card.capabilities, \"pushNotifications\", False)\n",
    "    push_url = (urllib.parse.urljoin(f\"http://{host}:{port}\", \"/notify\")\n",
    "                if can_push and host and port else None)\n",
    "    schemes = getattr(card.authentication, \"supportedSchemes\", [\"bearer\"])\n",
    "\n",
    "    # Assemble payload\n",
    "    payload = {\n",
    "        \"id\": uuid4().hex,\n",
    "        \"sessionId\": session,\n",
    "        \"acceptedOutputModes\": card.defaultOutputModes,\n",
    "        \"message\": {\"role\": \"user\", \"parts\": parts},\n",
    "        **({\"pushNotification\": {\"url\": push_url,\n",
    "                                 \"authentication\": {\"schemes\": schemes}}}\n",
    "           if push_url else {})\n",
    "    }\n",
    "\n",
    "    # Dispatch, letting the card decide streaming vs one-shot\n",
    "    stream = getattr(card.capabilities, \"streaming\", False)\n",
    "    return await _send_payload(client, card, session, payload, stream)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "617d27e5",
   "metadata": {},
   "source": [
    "#### 4.3. Orchestration Logic\n",
    "\n",
    "The `orchestrate` function coordinates the multi-agent interaction:\n",
    "\n",
    "1. **Planning Phase**: It queries the Planner Agent (via A2A) with the user's question and metadata about available skills from other agents. The Planner Agent returns a JSON plan (a list of `skill_id`s).\n",
    "\n",
    "2. **Execution Phase**: It iterates through the plan, calling the appropriate Skill Executor A2A Agents (e.g., Custom Tool Agent's skills) for each step.\n",
    "\n",
    "3. **Composition Phase**: Finally, it sends the original question and the collected results from the execution phase to the Composer A2A Agent to generate a polished, human-readable response."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2e5e4ec3",
   "metadata": {},
   "outputs": [],
   "source": [
    "async def orchestrate(\n",
    "    agent_manager: AgentManager,\n",
    "    question: str,\n",
    "    push: bool = False,\n",
    "    push_receiver: str = \"http://localhost:5000\",\n",
    ") -> str:\n",
    "    # Unpack orchestrator info\n",
    "    orch_url, orch_card, orch_client, orch_session = agent_manager.orchestrator\n",
    "\n",
    "    # Optionally start push listener\n",
    "    host = port = None\n",
    "    if push:\n",
    "        parsed = urllib.parse.urlparse(push_receiver)\n",
    "        host, port = parsed.hostname, parsed.port\n",
    "        auth = PushNotificationReceiverAuth()\n",
    "        await auth.load_jwks(f\"{orch_url}/.well-known/jwks.json\")\n",
    "        PushNotificationListener(host, port, auth).start()\n",
    "\n",
    "    # --- Planning Phase ---\n",
    "    print(\"\\n\\033[1;33m=========== 🧠 Planning Phase ===========\\033[0m\")\n",
    "    # Build skill metadata\n",
    "    skills_meta = _build_skill_meta(agent_manager)\n",
    "    plan_instructions = (\n",
    "        \"You are an orchestration assistant.\\n\"\n",
    "        \"Available skills (id & name & description & tags & examples):\\n\"\n",
    "        f\"{json.dumps(skills_meta, indent=2)}\\n\\n\"\n",
    "        \"When given a user question, respond _only_ with a JSON array of objects, \"\n",
    "        \"each with key `skill_id`, without any surrounding object. You may be asked to write single or multiple skills.\\n\"\n",
    "        \"For example for multiple tools:\\n\"\n",
    "        \"[\"\n",
    "        \"{\\\"skill_id\\\": \\\"tool_1\\\"}, \"\n",
    "        \"{\\\"skill_id\\\": \\\"tool_2\\\"}\"\n",
    "        \"]\"\n",
    "    )\n",
    "    combined = plan_instructions + \"\\n\\nUser question: \" + question\n",
    "    raw = await _send_task(agent_manager, orch_client, orch_card, orch_session, combined, push=push, host=host, port=port)\n",
    "    print(f\"Raw plan ➡️ {raw}\")\n",
    "    try:\n",
    "        plan = json.loads(raw[: raw.rfind(\"]\") + 1])\n",
    "    except ValueError:\n",
    "        print(\"\\033[31mPlan parse failed, fixing invalid JSON...\\033[0m\")\n",
    "        fixer = \"Fix this json to be valid: \" + raw\n",
    "        fixed = await _send_task(agent_manager, orch_client, orch_card, orch_session, fixer, push=push, host=host, port=port)\n",
    "        plan = json.loads(fixed)\n",
    "    print(f\"\\n\\033[1;32mFinal plan ➡️ {plan}\\033[0m\")\n",
    "\n",
    "    # --- Execution Phase ---\n",
    "    print(\"\\n\\033[1;33m=========== ⚡️ Execution Phase ===========\\033[0m\")\n",
    "    parts = []\n",
    "    for i, step in enumerate(plan, 1):\n",
    "        sid = step[\"skill_id\"]\n",
    "        inp = json.dumps(step.get(\"input\", {}))\n",
    "        print(f\"➡️ Step {i}: {sid}({inp})\")\n",
    "\n",
    "        info = agent_manager.skills.get(sid)\n",
    "        if not info:\n",
    "            print(f\"\\033[31mNo executor for '{sid}', skipping.\\033[0m\")\n",
    "            parts.append({\"skill_id\": sid, \"output\": None})\n",
    "            continue\n",
    "\n",
    "        _, skill_card, skill_client, skill_sess = info\n",
    "        out = await _send_task(agent_manager, skill_client, skill_card, skill_sess, f\"{sid}({inp})\", push=push, host=host, port=port)\n",
    "        print(f\"   ✅ → {out}\")\n",
    "        parts.append({\"skill_id\": sid, \"output\": out})\n",
    "\n",
    "    # --- Composing Answer ---\n",
    "    print(\"\\n\\033[1;33m=========== 🛠️ Composing Answer ===========\\033[0m\")\n",
    "    comp_prompt = (\n",
    "        f\"Using the following information: {json.dumps(parts)}, \"\n",
    "        f\"write a clear and human-friendly response to the question: '{question}'. \"\n",
    "        \"Keep it concise and easy to understand and respond like a human with character. \"\n",
    "        \"Only use the information provided. If you cannot answer the question, say 'I don't know'. \"\n",
    "        \"Never show any code or JSON or Markdown, just the answer.\\n\\n\"\n",
    "    )\n",
    "    _, write_card, write_client, write_sess = agent_manager.skills[\"writing_agent\"]\n",
    "    final = await _send_task(agent_manager, write_client, write_card, write_sess, comp_prompt, push=push, host=host, port=port)\n",
    "\n",
    "    print(\"\\n\\033[1;36m🎉 FINAL ANSWER\\033[0m\")\n",
    "    print(final)\n",
    "    print(\"\\033[1;36m====================================\\033[0m\")\n",
    "    return final"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "937dbee6",
   "metadata": {},
   "source": [
    "### 5. Running the Orchestration\n",
    "Now we define the URLs for our orchestrator (Planner Agent) and skill executor agents (Custom Tool Agent, Composer Agent). We then initialize the `AgentManager` and call the `orchestrate` function with sample questions.\n",
    "\n",
    "The `AgentManager` uses the first URL for the orchestrator/planner and the rest for skill executors. The `orchestrate` function will then:\n",
    "\n",
    "1. Query the Planner Agent with the user's question and the list of available skills.\n",
    "\n",
    "2. The Planner Agent returns a plan (e.g. `[{'skill_id': 'random_number_tool'}]`).\n",
    "\n",
    "3. The `orchestrate` function executes the plan by calling the specified skill agents.\n",
    "\n",
    "4. Finally, it sends the original question and the skill outputs to the Composer Agent to generate a polished, human-readable response."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "19d1fa85",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1;36m===================== 🛰️ Connected Agents =====================\u001b[0m\n",
      "Orchestrator: http://localhost:10020 (Orchestration Agent)\n",
      "Executors:\n",
      "  • random_number_tool -> http://localhost:10021 (Custom Agent)\n",
      "  • date_tool -> http://localhost:10021 (Custom Agent)\n",
      "  • writing_agent -> http://localhost:10022 (Writing Agent)\n",
      "\u001b[1;36m===============================================================\u001b[0m\n",
      "\n",
      "\u001b[1;33m=========== 🧠 Planning Phase ===========\u001b[0m\n",
      "Raw plan ➡️ [\n",
      "  {\"skill_id\": \"date_tool\"},\n",
      "  {\"skill_id\": \"random_number_tool\"},\n",
      "  {\"skill_id\": \"random_number_tool\"},\n",
      "  {\"skill_id\": \"random_number_tool\"},\n",
      "  {\"skill_id\": \"random_number_tool\"},\n",
      "  {\"skill_id\": \"random_number_tool\"}\n",
      "]\n",
      "\n",
      "\u001b[1;32mFinal plan ➡️ [{'skill_id': 'date_tool'}, {'skill_id': 'random_number_tool'}, {'skill_id': 'random_number_tool'}, {'skill_id': 'random_number_tool'}, {'skill_id': 'random_number_tool'}, {'skill_id': 'random_number_tool'}]\u001b[0m\n",
      "\n",
      "\u001b[1;33m=========== ⚡️ Execution Phase ===========\u001b[0m\n",
      "➡️ Step 1: date_tool({})\n",
      "   ✅ → {\n",
      "    \"type\": \"function\",\n",
      "    \"name\": \"date_tool\",\n",
      "    \"parameters\": {}\n",
      "}Tool:date_tool Args:{}Tool:date_tool Response:\"2025-06-06\"{\"type\": \"function\", \"name\": \"date_tool\", \"parameters\": {}}Tool:date_tool Args:{}Tool:date_tool Response:\"2025-06-06\"The output of the function call is today's date in YYYY-MM-DD format.\n",
      "➡️ Step 2: random_number_tool({})\n",
      "   ✅ → {\n",
      "    \"type\": \"function\",\n",
      "    \"name\": \"random_number_tool\",\n",
      "    \"parameters\": {}\n",
      "}Tool:random_number_tool Args:{}Tool:random_number_tool Response:87{\"type\": \"function\", \"name\": \"random_number_tool\", \"parameters\": {}}Tool:random_number_tool Args:{}Tool:random_number_tool Response:96```\n",
      "{\"type\": \"function\", \"name\": \"random_number_tool\", \"parameters\": {}}\n",
      "➡️ Step 3: random_number_tool({})\n",
      "   ✅ → {\n",
      "    \"type\": \"function\",\n",
      "    \"name\": \"random_number_tool\",\n",
      "    \"parameters\": {}\n",
      "}Tool:random_number_tool Args:{}Tool:random_number_tool Response:91The random number generated is 91.\n",
      "➡️ Step 4: random_number_tool({})\n",
      "   ✅ → {\n",
      "    \"type\": \"function\",\n",
      "    \"name\": \"random_number_tool\",\n",
      "    \"parameters\": {}\n",
      "}Tool:random_number_tool Args:{}Tool:random_number_tool Response:17{\"type\": \"function\", \"name\": \"random_number_tool\", \"parameters\": {}}Tool:random_number_tool Args:{}Tool:random_number_tool Response:49```\n",
      "{\"type\": \"function\", \"name\": \"random_number_tool\", \"parameters\": {}}\n",
      "➡️ Step 5: random_number_tool({})\n",
      "   ✅ → {\n",
      "    \"type\": \"function\",\n",
      "    \"name\": \"random_number_tool\",\n",
      "    \"parameters\": {}\n",
      "}Tool:random_number_tool Args:{}Tool:random_number_tool Response:22{\"type\": \"function\", \"name\": \"random_number_tool\", \"parameters\": {}}Tool:random_number_tool Args:{}Tool:random_number_tool Response:37{\"type\": \"function\", \"name\": \"random_number_tool\", \"parameters\": {}}\n",
      "➡️ Step 6: random_number_tool({})\n",
      "   ✅ → {\n",
      "    \"type\": \"function\",\n",
      "    \"name\": \"random_number_tool\",\n",
      "    \"parameters\": {}\n",
      "}Tool:random_number_tool Args:{}Tool:random_number_tool Response:48{\"type\": \"function\", \"name\": \"random_number_tool\", \"parameters\": {}}Tool:random_number_tool Args:{}Tool:random_number_tool Response:15{\"type\": \"function\", \"name\": \"random_number_tool\", \"parameters\": {}}\n",
      "\n",
      "\u001b[1;33m=========== 🛠️ Composing Answer ===========\u001b[0m\n",
      "\n",
      "\u001b[1;36m🎉 FINAL ANSWER\u001b[0m\n",
      "Today's date is June 6th, 2025.\n",
      "\n",
      "Here are five random numbers for you: 87, 91, 17, 22, and 48.\n",
      "\u001b[1;36m====================================\u001b[0m\n",
      "\n",
      "\u001b[1;33m=========== 🧠 Planning Phase ===========\u001b[0m\n",
      "Raw plan ➡️ [{\"skill_id\": \"date_tool\"}]\n",
      "\n",
      "\u001b[1;32mFinal plan ➡️ [{'skill_id': 'date_tool'}]\u001b[0m\n",
      "\n",
      "\u001b[1;33m=========== ⚡️ Execution Phase ===========\u001b[0m\n",
      "➡️ Step 1: date_tool({})\n",
      "   ✅ → {\n",
      "    \"type\": \"function\",\n",
      "    \"name\": \"date_tool\",\n",
      "    \"parameters\": {}\n",
      "}Tool:date_tool Args:{}Tool:date_tool Response:\"2025-06-06\"{\"type\": \"function\", \"name\": \"date_tool\", \"parameters\": {}}Tool:date_tool Args:{}Tool:date_tool Response:\"2025-06-06\"The output of the function call is today's date in YYYY-MM-DD format.\n",
      "\n",
      "\u001b[1;33m=========== 🛠️ Composing Answer ===========\u001b[0m\n",
      "\n",
      "\u001b[1;36m🎉 FINAL ANSWER\u001b[0m\n",
      "Today's date is June 6th, 2025.\n",
      "\u001b[1;36m====================================\u001b[0m\n",
      "\n",
      "\u001b[1;33m=========== 🧠 Planning Phase ===========\u001b[0m\n",
      "Raw plan ➡️ [\n",
      "  {\"skill_id\": \"random_number_tool\"}\n",
      "]\n",
      "\n",
      "\u001b[1;32mFinal plan ➡️ [{'skill_id': 'random_number_tool'}]\u001b[0m\n",
      "\n",
      "\u001b[1;33m=========== ⚡️ Execution Phase ===========\u001b[0m\n",
      "➡️ Step 1: random_number_tool({})\n",
      "   ✅ → {\n",
      "    \"type\": \"function\",\n",
      "    \"name\": \"random_number_tool\",\n",
      "    \"parameters\": {}\n",
      "}Tool:random_number_tool Args:{}Tool:random_number_tool Response:31{\"type\": \"function\", \"name\": \"random_number_tool\", \"parameters\": {}}Tool:random_number_tool Args:{}Tool:random_number_tool Response:20{\"type\": \"function\", \"name\": \"random_number_tool\", \"parameters\": {}}\n",
      "\n",
      "\u001b[1;33m=========== 🛠️ Composing Answer ===========\u001b[0m\n",
      "\n",
      "\u001b[1;36m🎉 FINAL ANSWER\u001b[0m\n",
      "Here's your randomly generated number: 31.\n",
      "\u001b[1;36m====================================\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "ORCHESTRATOR_URL = \"http://localhost:10020\"\n",
    "EXECUTOR_URLS    = [\"http://localhost:10021\", \"http://localhost:10022\"]\n",
    "URLS             = [ORCHESTRATOR_URL, *EXECUTOR_URLS]\n",
    "\n",
    "_agent_manager = AgentManager(URLS)\n",
    "orch_url, orch_card, *_ = _agent_manager.orchestrator\n",
    "\n",
    "print(\"\\n\\033[1;36m===================== 🛰️ Connected Agents =====================\\033[0m\")\n",
    "print(f\"Orchestrator: {orch_url} ({orch_card.name})\")\n",
    "print(\"Executors:\")\n",
    "for sid, (u, card, *_) in _agent_manager.skills.items():\n",
    "    print(f\"  • {sid} -> {u} ({card.name})\")\n",
    "print(\"\\033[1;36m===============================================================\\033[0m\")\n",
    "\n",
    "questions = [ \n",
    "    \"Get todays date then generate five random numbers\",\n",
    "    \"Get todays date?\",\n",
    "    \"I want one random number\",\n",
    "    ]\n",
    "\n",
    "for question in questions:\n",
    "    await orchestrate(_agent_manager, question)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32d32564",
   "metadata": {},
   "source": [
    "## 6. Wrapping Up & Future Directions\n",
    "\n",
    "We've successfully orchestrated a team of specialized Llama Stack agents, showcasing how they can collaborate via the A2A protocol to tackle complex queries.\n",
    "\n",
    "**What We Achieved:**\n",
    "\n",
    "* We configured the Llama Stack environment and designed three distinct agents: a `Planner`, a `Custom Tool Agent` (with date/random number skills), and a `Composer`.\n",
    "\n",
    "* Each agent was made accessible through A2A, with an `AgentManager` and an `orchestrate` function guiding their interaction to deliver user-friendly answers.\n",
    "\n",
    "The key insight is the power of modular, specialized agents communicating through a standard protocol, allowing for flexible and scalable AI system development.\n",
    "\n",
    "### Future Directions:\n",
    "\n",
    "Inspired? Here are a few ways to build on this foundation:\n",
    "\n",
    "* **Refine & Expand**: Experiment with agent instructions or add new tools and specialized agents to the team.\n",
    "\n",
    "* **Boost Orchestration**: Explore more dynamic planning, such as conditional logic or inter-agent feedback loops.\n",
    "\n",
    "* **Challenge the System**: Test with increasingly complex queries to push the boundaries of the current setup.\n",
    "\n",
    "This notebook serves as a stepping stone into the exciting world of multi-agent AI. We hope it empowers you to build even more sophisticated applications. Happy coding!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
